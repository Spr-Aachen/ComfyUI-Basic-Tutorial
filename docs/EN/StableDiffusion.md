# Stable Diffusion


## 概述

### 介绍
Stable Diffusion（稳定扩散）严格说来它是一个由几个组件（模型）构成的系统，而非单独的一个模型。

### 工作原理
以文生图为例，当我们输入一句 prompt 后，比如“Cat, standing on the castle”，Stable Diffusion 会生成一张猫猫站在城堡的图。生成的过程经过三个大的步骤：

![](/docs/media/Text2Img.png)

1. 首先，用户输入的 Prompt 会被一个叫 Text Encoder（文本编译器）的东西编译成一个个的词特征向量。此步骤下会输出 77 个等长的向量，每个向量包含 768 个维度。后续我会展开讲讲这些向量的作用，现在各位可以简单将其理解为「将文本转化为机器能识别的多组数字」。
2. 接着，这些特征向量会和一张随机图（可以简单理解这是一张布满电子雪花的图，或充满信息噪声的图），一起放到 Image Information Creator 里。在这一步，机器会将这些特征向量和随机图先转化到一个 Latent Space（潜空间）里，然后根据这些特征向量，将随机图「降噪」为一个「中间产物」。你可以简单理解，此时的「中间产物」是人类看不懂的「图」，是一堆数字，但此时这个中间产物所呈现的信息已经是是一只站在城堡上的猫了。
3. 最后，这个中间产物会被 Image Decoder（图片解码器）解码成一张真正的图片。

简单理解，就是用户输入了一段 Prompt 指令，机器会按照这个指令，在一个潜空间里，将一张随机图降噪为一张符合指令的图片。


## Image Information Creator

### 
我们就来展开讲讲整个降噪的过程。

首先整个降噪的过程会在一个 Latent Space（潜空间）里进行，然后会进行多 Steps（步）的 Denoise（降噪），你可以对这个 Steps 进行调整，一般越多图片质量也会好，但时间也会越久。

### 
下图是单个 Denoise 过程的可视化：：

![](/docs/media/Denoise.png)

1. 首先，在 Denoise 里有一个 Noise Predictor（噪音预测器），顾名思义，它就是能预测出随机图里包含什么噪音的模型。除了输入随机图和 Prompt 的词特征向量外，还需要输入当前的 Step 数。虽然在上面的可视化流程中，你会看到很多个 Denoise，但实际程序运行的是同一个 Denoise，所以需要将 Step 告知 Noise Predictor 让其知道正在进行哪一步的预测。
2. 然后，我们先来看橙色的线，Noise Predictor 会使用随机图（比如一张 4 X 4 的图）和 Prompt 的词特征向量预测出一张噪声图 B。注意，这里不是根据预测输出实际的图，而是一张噪声图。换句话来说，Noise Predictor 是根据词向量预测这张随机图里有哪些不需要的噪声。如果拿前面的雕刻的例子来类比，它输出的是雕刻雕像所不需要的废料。于此同时，Noise Predictor 还会使用不包含 Prompt 的词特征向量预测出一张噪音图 C（也就是图中的蓝色线）。
3. 接着，Denoise 会拿噪音图 B 和 C 相减得出图 D。我们用简单的数学解释下这张图是啥。首先，图 B 是用 Prompt 加随机图预测的噪声，简单理解，就包含了「根据 Prompt 预测的噪声」+「根据随机图预测的噪声」，而 C 则是「根据随机图预测的噪声」，B 减 C 就等于「根据 Prompt 预测的噪声」。
再之后，Denoise 会将 D 噪声放大，一般就是会乘以一个系数，这个系数在一些 Stable Diffusion 里会以 CFG、CFG Scale 或者 Guidance Scale 表示。接着再拿这张放大后的图与噪声图 C 相加，得到图 E。这样做的原因是为了提高图片生成的准确性，所以通过乘以一个系数，来刻意提高「根据 Prompt 预测的噪声」的权重。如果没有这一步，生成的图片就跟 Prompt 没那么相关了。这个方法也被称为 Classifier Free Guidance（无分类引导法）。
最后，Denoise 会将图 A 减去图 E，得出一张新的图。也就是我前面提到的「雕刻」的过程，去掉不需要的噪声。


## Latent Space
潜在空间是指在机器学习和深度学习中，用于表示数据的低维空间。它是通过对原始数据进行编码和降维得到的一组潜在变量。潜在空间的维度通常比原始数据的维度低，因此可以提取出数据中最重要的特征和结构。

简单理解就是潜空间会将图片编码成一堆数字，同时对这些数字进行压缩。让我们通过可视化的方式看看这个过程：

![](/docs/media/LantentSpace.png)


## Image Decoder

###
图片会先被一个 Image Encoder 编码成一组数据，并被压缩，如果用像素的角度来衡量这个数据压缩的效果，原图可能是一张 512 X 512 的图，压缩后变成了 64 X 64，数据极大地减少了，最后再使用 Image Decoder 还原即可。而这个 Encoder 加 Decoder 的组件，也被称为 Variational Auto Encoder（变分自编码器）简称 VAE 。所以这个 Image Decoder 在一些产品里，也叫 VAE Decoder。

### 好处：
- 首先当然是效率提升了非常多。使用 VAE 后，即使民用的 GPU 也能以相对较快的速度，完成降噪运算。同时训练模型的时间也会更短。
- 另外，潜在空间的维度通常比原始图像的维度低得多，这意味着它可以更有效地表示图像的特征。通过在潜在空间中进行操作和插值，可以对图像进行更精细的控制和编辑。这使得在生成图像时可以更好地控制图像的细节和风格，从而提高生成图像的质量和逼真度。

### 坏处：
- 经过编码，然后再将数据还原会导致一些数据丢失。
- 加上潜在空间的维度较低，它可能无法完全捕捉原始数据中的所有细节和特征。最终导致还原的图片比较奇怪。


## Text Encoder
提到过 Text Encoder（文本编译器）会将输入的 Prompt 编译成一个个的词特征向量。此步骤下会输出 77 个等长的向量，每个向量包含 768 个维度。

目前 Stable Diffusion 常用的 Text Encoder 用的是 OpenAI 开源的 CLIP 模型，全称为 Contrastive Language Image Pre-training（对比语言图像预训练）。我们照例先画个图：

![](/docs/media/TextEncoder.png)

1. 首先，这个 CLIP 也有一个 Text Encoder，会将文本转化为一个特征向量。
2. 然后它还有一个 Image Encoder 会将图片也转成各种特征向量。如果这两个向量越近，意味着这个描述，越接近图片的内容；反之越远，则越不相关。